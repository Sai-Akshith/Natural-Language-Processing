{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fdf092-35e6-4112-9562-3087bf722682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                            Sentences for Training   \\\n",
      "0           NaN               I deposited my paycheck at the bank.   \n",
      "1           NaN  The bank of the river was eroded by the heavy ...   \n",
      "2           NaN                She works at the bank as a teller.    \n",
      "3           NaN     Let's have a picnic by the bank of the river.    \n",
      "4           NaN            The bank approved my loan application.    \n",
      "..          ...                                                ...   \n",
      "96          NaN  I need to update my contact information with t...   \n",
      "97          NaN  The bank provides online banking services for ...   \n",
      "98          NaN  The beavers constructed a dam along the bank o...   \n",
      "99          NaN  I need to check my transaction history at the ...   \n",
      "100         NaN  She works as a financial consultant at the bank.    \n",
      "\n",
      "    Sense of the word \"Bank\"  \n",
      "0      Financial Institution  \n",
      "1               River Border  \n",
      "2      Financial Institution  \n",
      "3               River Border  \n",
      "4      Financial Institution  \n",
      "..                       ...  \n",
      "96                         ?  \n",
      "97                         ?  \n",
      "98                         ?  \n",
      "99                         ?  \n",
      "100                        ?  \n",
      "\n",
      "[101 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importing, Reading & Printing excel files\n",
    "\n",
    "import pandas as pd\n",
    "excel_data = pd.read_excel(\"C:\\\\Users\\\\91939\\\\Dropbox\\\\Sem 5\\\\NLP\\\\LAB\\\\Bank.xlsx\")\n",
    "print(excel_data)\n",
    "excel_data.to_csv('output_file.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8648524c-f851-4853-afac-1c2b3662bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        v1                                                 v2 Unnamed: 2  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "...    ...                                                ...        ...   \n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
      "5568   ham              Will ÃŒ_ b going to esplanade fr home?        NaN   \n",
      "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
      "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
      "5571   ham                         Rofl. Its true to its name        NaN   \n",
      "\n",
      "     Unnamed: 3 Unnamed: 4  \n",
      "0           NaN        NaN  \n",
      "1           NaN        NaN  \n",
      "2           NaN        NaN  \n",
      "3           NaN        NaN  \n",
      "4           NaN        NaN  \n",
      "...         ...        ...  \n",
      "5567        NaN        NaN  \n",
      "5568        NaN        NaN  \n",
      "5569        NaN        NaN  \n",
      "5570        NaN        NaN  \n",
      "5571        NaN        NaN  \n",
      "\n",
      "[5572 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importing, Reading & Printing csv files\n",
    "import pandas as pd\n",
    "\n",
    "# Specify encoding to handle special characters\n",
    "csv_data = pd.read_csv(\"C:\\\\Users\\\\91939\\\\Dropbox\\\\Sem 5\\\\NLP\\\\LAB\\\\Vollala Sai Akshith - spam.csv\", encoding='ISO-8859-1')\n",
    "print(csv_data)\n",
    "\n",
    "# Saving the file with a specified encoding\n",
    "csv_data.to_csv('csv_output.csv', index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b4a9a11-c89a-4fe3-96dd-cd867ea7c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepalLength  sepalWidth  petalLength  petalWidth    species\n",
      "0            5.1         3.5          1.4         0.2     setosa\n",
      "1            4.9         3.0          1.4         0.2     setosa\n",
      "2            4.7         3.2          1.3         0.2     setosa\n",
      "3            4.6         3.1          1.5         0.2     setosa\n",
      "4            5.0         3.6          1.4         0.2     setosa\n",
      "..           ...         ...          ...         ...        ...\n",
      "145          6.7         3.0          5.2         2.3  virginica\n",
      "146          6.3         2.5          5.0         1.9  virginica\n",
      "147          6.5         3.0          5.2         2.0  virginica\n",
      "148          6.2         3.4          5.4         2.3  virginica\n",
      "149          5.9         3.0          5.1         1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_data = pd.read_json(\"C:\\\\Users\\\\91939\\\\Dropbox\\\\Sem 5\\\\NLP\\\\LAB\\\\iris.json\")\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c71b51-6f21-4998-b44d-3054f3f9ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error loading averaged_perception_tagger: Package\n",
      "[nltk_data]     'averaged_perception_tagger' not found in index\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffd4c00-d9b7-47c3-a13d-f0012f8421a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:  ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLP']\n",
      "Sentence Tokens:  ['Hello!', 'How are you doing today?', \"I'm learning NLP\"]\n",
      "good.n.01 benefit\n",
      "good.n.02 moral excellence or admirableness\n",
      "good.n.03 that which is pleasing or valuable or useful\n",
      "commodity.n.01 articles of commerce\n",
      "good.a.01 having desirable or positive qualities especially those suitable for a thing specified\n",
      "full.s.06 having the normally expected amount\n",
      "good.a.03 morally admirable\n",
      "estimable.s.02 deserving of esteem and respect\n",
      "beneficial.s.01 promoting or enhancing well-being\n",
      "good.s.06 agreeable or pleasing\n",
      "good.s.07 of moral excellence\n",
      "adept.s.01 having or showing knowledge and skill and aptitude\n",
      "good.s.09 thorough\n",
      "dear.s.02 with or in a close or intimate relationship\n",
      "dependable.s.04 financially sound\n",
      "good.s.12 most suitable or right for a particular purpose\n",
      "good.s.13 resulting favorably\n",
      "effective.s.04 exerting force or influence\n",
      "good.s.15 capable of pleasing\n",
      "good.s.16 appealing to the mind\n",
      "good.s.17 in excellent physical condition\n",
      "good.s.18 tending to promote physical well-being; beneficial to health\n",
      "good.s.19 not forged\n",
      "good.s.20 not left to spoil\n",
      "good.s.21 generally admired\n",
      "well.r.01 (often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\n",
      "thoroughly.r.02 completely and absolutely (`good' is sometimes used informally for `thoroughly')\n",
      "Examples:  ['for your own good', \"what's the good of worrying?\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = \"Hello! How are you doing today? I'm learning NLP\"\n",
    "word_tokens = word_tokenize(text)\n",
    "print('Word Tokens: ', word_tokens)\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print('Sentence Tokens: ', sentence_tokens)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = wordnet.synsets('good')\n",
    "for syn in synonyms:\n",
    "    print(syn.name(), syn.definition())\n",
    "\n",
    "print(\"Examples: \", synonyms[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc412e9-edba-4c93-bd81-fe96768897f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words:  ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a sample sentence, showing off the stop words filtration\"\n",
    "word_tokens = word_tokenize(text)\n",
    "# for word in word_tokens:\n",
    "#     if word.lower() not in stop_words:\n",
    "#         print(word, \" \",  end = \"\")\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "print('Filtered Words: ', filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03977f97-6221-4433-8232-fc1394cc49fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words:  ['run', 'ran', 'runner', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words = ['running', 'ran', 'runner', 'easily', 'fairly']\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print('Stemmed Words: ', stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af8d4259-bd6d-4ce1-b4cf-95aeb773d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('running', pos = 'v'))\n",
    "print(lemmatizer.lemmatize('better', pos = 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "399df288-6fe8-42b5-8aa8-a4e358cc4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97be309b-50f1-4f0b-add3-a2c7f996f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  44th/JJ\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION US/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "text = 'Barack Obama was the 44th President of the US'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "entities = ne_chunk(pos_tags)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdb7236c-57a1-4862-90e2-67251af3be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             S                     \n",
      "      _______|_______               \n",
      "     |               VP            \n",
      "     |        _______|___           \n",
      "     |       |           PP        \n",
      "     |       |    _______|___       \n",
      "     NP      |   |           NP    \n",
      "  ___|___    |   |        ___|___   \n",
      "Det      N   V   P      Det      N \n",
      " |       |   |   |       |       |  \n",
      "the     cat sat  on     the     mat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "grammer = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    Det -> 'the'\n",
    "    N -> 'cat' | 'mat'\n",
    "    V -> 'sat'\n",
    "    P -> 'on'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammer)\n",
    "sentence = 'the cat sat on the mat'.split()\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d7b66-c9cf-476c-b7f4-3bdb5647ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
